# <img src="_static/lighton_small.png" width=60/>Adversarial Robustness by Design throughAnalog Computing and Synthetic Gradients

[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)  [![Twitter](https://img.shields.io/twitter/follow/LightOnIO?style=social)](https://twitter.com/LightOnIO)

We study a defense mechanism involving an Optical Processing Units and Direct Feedback Alignment algorithm against adversarial attacks. We show how such defense provides robustness vs white-box attacks, transfer attacks and black-box attacks. Finally we provide an ablation study to show which part of our defense mechanism is responsable in providing robustness for each kind of attack. 

**Remember to fill in the Description, Website and Topics of the repository.**

## Requirements

- A `requirements.txt` file is available at the root of this repository, specifying the required packages for all of our experiments; 

## Reproducing our results

- Run `./experiments` for results reproduction. 

## <img src="_static/lighton_cloud_small.png" width=120/> Access to Optical Processing Units

To request access to LightOn Cloud and try our photonic co-processor, please visit: https://cloud.lighton.ai/

For researchers, we also have a LightOn Cloud for Research program, please visit https://cloud.lighton.ai/lighton-research/ for more information.

## Citation \[ OPTIONAL \]

If you found this code and findings useful in your research, please consider citing:
<Bibtex for citation>
  
